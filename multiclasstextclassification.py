# -*- coding: utf-8 -*-
"""MulticlassTextClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pZfKFhtIJJ71zIPe8oC0kNhyrn4q2Zpd
"""

from google.colab import files
upload=files.upload()

import pandas as pd
df = pd.read_csv('imdb_indonesian_movies_2 (1).csv')
df = df.drop(columns=['judul_film'])

df.head()

#proses one hot encoding dan membuat dataframe baru
kategori = pd.get_dummies(df.genre)
df_baru = pd.concat([df, kategori], axis=1)
df_baru = df_baru.drop(columns='genre')
df_baru

#agar mampu diproses oleh model, ubah nilai dataframe kedala tipe numpy array mengunaka atribut values

sinopsis = df_baru['ringkasan_sinopsis'].values
label = df_baru[['Drama', 'Horor', 'Komedi', 'Laga', 'Romantis']].values

#membagi data training dan testing
from sklearn.model_selection import train_test_split
sinopsis_latih, sinopsis_test, label_latih, label_test = train_test_split(sinopsis, label, test_size=0.2)

#mengubah dataset kedalam bilangan numerik denga rokenizer
#kemudia konversi setiap sampel menjadi sequence

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(sinopsis_latih) 
tokenizer.fit_on_texts(sinopsis_test)
 
sekuens_latih = tokenizer.texts_to_sequences(sinopsis_latih)
sekuens_test = tokenizer.texts_to_sequences(sinopsis_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

#arsitektur model
#menggunakan layer embedding dengan dimensi embedding sebesar 16
#dimensi input sebesar nilai bum_words pada objek tokenizer
#panggil fungsi compile dan tentukan optimizer serta loss function yang akan dipakai pada model

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

#latih model dengan funsi fit()

num_epochs = 30
history = model.fit(padded_latih, label_latih, epochs=num_epochs, 
                    validation_data=(padded_test, label_test), verbose=2)